{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting defective products - ASML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the raw dataset and Performing feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.108216</td>\n",
       "      <td>33.094774</td>\n",
       "      <td>15.821643</td>\n",
       "      <td>-1.102204</td>\n",
       "      <td>25.651303</td>\n",
       "      <td>6.733467</td>\n",
       "      <td>-1.102204</td>\n",
       "      <td>5.991984</td>\n",
       "      <td>1277.312928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.733467</td>\n",
       "      <td>61.647664</td>\n",
       "      <td>36.352705</td>\n",
       "      <td>-0.300601</td>\n",
       "      <td>64.729459</td>\n",
       "      <td>13.126253</td>\n",
       "      <td>-0.300601</td>\n",
       "      <td>7.975952</td>\n",
       "      <td>1309.375615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.312625</td>\n",
       "      <td>-57.273481</td>\n",
       "      <td>19.899800</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>25.651303</td>\n",
       "      <td>8.016032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.148297</td>\n",
       "      <td>1261.211704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.911824</td>\n",
       "      <td>54.004565</td>\n",
       "      <td>40.420842</td>\n",
       "      <td>-1.302605</td>\n",
       "      <td>68.236473</td>\n",
       "      <td>1.703407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.605210</td>\n",
       "      <td>1207.962228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100200</td>\n",
       "      <td>65.841901</td>\n",
       "      <td>31.432866</td>\n",
       "      <td>-2.304609</td>\n",
       "      <td>50.701403</td>\n",
       "      <td>4.609218</td>\n",
       "      <td>-2.304609</td>\n",
       "      <td>12.164329</td>\n",
       "      <td>1160.177286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1         f2         f3        f4         f5         f6        f7  \\\n",
       "0   4.108216  33.094774  15.821643 -1.102204  25.651303   6.733467 -1.102204   \n",
       "1  16.733467  61.647664  36.352705 -0.300601  64.729459  13.126253 -0.300601   \n",
       "2   6.312625 -57.273481  19.899800  0.701403  25.651303   8.016032       NaN   \n",
       "3  -5.911824  54.004565  40.420842 -1.302605  68.236473   1.703407       NaN   \n",
       "4   0.100200  65.841901  31.432866 -2.304609  50.701403   4.609218 -2.304609   \n",
       "\n",
       "          f8       target  \n",
       "0   5.991984  1277.312928  \n",
       "1   7.975952  1309.375615  \n",
       "2  14.148297  1261.211704  \n",
       "3  12.605210  1207.962228  \n",
       "4  12.164329  1160.177286  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv('xl_data_science.csv')\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63080"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_raw.dropna(axis=0,how='any'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many missing values in dataset, if I simply dropped the rows with missing value, the dataset will have about 60% remaining only, so it is not a feasible way.\n",
    "#### There are several ways to fill these missing values:\n",
    "<br>1. Fill gaps forward or backward, or linear interpolation at missing datapoints. But I don't think it is a feasible way in this dataset, cause each row is representing a single product and they are independent.<br/>\n",
    "<br>2. Replace missing value with a specific scalar value - mean value of columns or a value didn't appear in the column(indicating missing value), depends on the data distribution and the model used later.<br/>\n",
    "#### Here, I used mean value filling first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.108216</td>\n",
       "      <td>33.094774</td>\n",
       "      <td>15.821643</td>\n",
       "      <td>-1.102204</td>\n",
       "      <td>25.651303</td>\n",
       "      <td>6.733467</td>\n",
       "      <td>-1.102204</td>\n",
       "      <td>5.991984</td>\n",
       "      <td>1277.312928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.733467</td>\n",
       "      <td>61.647664</td>\n",
       "      <td>36.352705</td>\n",
       "      <td>-0.300601</td>\n",
       "      <td>64.729459</td>\n",
       "      <td>13.126253</td>\n",
       "      <td>-0.300601</td>\n",
       "      <td>7.975952</td>\n",
       "      <td>1309.375615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.312625</td>\n",
       "      <td>-57.273481</td>\n",
       "      <td>19.899800</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>25.651303</td>\n",
       "      <td>8.016032</td>\n",
       "      <td>0.345671</td>\n",
       "      <td>14.148297</td>\n",
       "      <td>1261.211704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.911824</td>\n",
       "      <td>54.004565</td>\n",
       "      <td>40.420842</td>\n",
       "      <td>-1.302605</td>\n",
       "      <td>68.236473</td>\n",
       "      <td>1.703407</td>\n",
       "      <td>0.345671</td>\n",
       "      <td>12.605210</td>\n",
       "      <td>1207.962228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100200</td>\n",
       "      <td>65.841901</td>\n",
       "      <td>31.432866</td>\n",
       "      <td>-2.304609</td>\n",
       "      <td>50.701403</td>\n",
       "      <td>4.609218</td>\n",
       "      <td>-2.304609</td>\n",
       "      <td>12.164329</td>\n",
       "      <td>1160.177286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1         f2         f3        f4         f5         f6        f7  \\\n",
       "0   4.108216  33.094774  15.821643 -1.102204  25.651303   6.733467 -1.102204   \n",
       "1  16.733467  61.647664  36.352705 -0.300601  64.729459  13.126253 -0.300601   \n",
       "2   6.312625 -57.273481  19.899800  0.701403  25.651303   8.016032  0.345671   \n",
       "3  -5.911824  54.004565  40.420842 -1.302605  68.236473   1.703407  0.345671   \n",
       "4   0.100200  65.841901  31.432866 -2.304609  50.701403   4.609218 -2.304609   \n",
       "\n",
       "          f8       target  \n",
       "0   5.991984  1277.312928  \n",
       "1   7.975952  1309.375615  \n",
       "2  14.148297  1261.211704  \n",
       "3  12.605210  1207.962228  \n",
       "4  12.164329  1160.177286  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_raw.fillna(data_raw.mean())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To understand the features, I plotted the distribution of every feature:\n",
    "<br>1. Most features are in a normal distribution, which is perfect.<br/>\n",
    "<br>2. Some features are in a positive skewed distribution, one way to reduce the skewness is to do log transform if necessary.<br/>\n",
    "<br>3. There are too many missing values in feature 'f7', so if I filled them with mean value, the frequency is much higher than other values. So it might be better to consider dropping this feature in training.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29eed48f940>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEFCAYAAADt1CyEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFGpJREFUeJzt3WGMXNdZh/Fn7Y29MR6blRg3atQ2QOFtqJoGUiXQxM1+\nSOraFEwrgqqokLSQusEoLQ0qNHEKEY4MUQjUKkmLm+BEbgWqS1Gx5DQobYztBtyWVHJoeCuXSkhI\nlbaRY28wa8v28mHutpPNzM7Y3p3ZOfv8JEt3zj27c+7Zmf8cn3vvmaGpqSkkSeVa0u8GSJLml0Ev\nSYUz6CWpcAa9JBXOoJekwg33uwGtjI9PtL0UaHR0BUePnuhlcwaGfdOa/dKefdPaoPZLvV4balU+\ncCP64eGl/W7CgmXftGa/tGfftFZavwxc0EuSzo1BL0mF6zhHHxG3ArdWD0eAK4HrgL8CpoDngM2Z\neTYibgM2AaeBrZm5JyIuBnYBa4AJ4JbMHJ/j45AktdFxRJ+ZOzNzLDPHgG8CdwAfB7Zk5lpgCNgY\nEZdU+64F1gHbImI5cDtwuKr7OLBlXo5EktRS11M3EfEW4I2Z+TfAVcC+atde4AbgauBgZp7MzGPA\nEeAKGqP/J2bUlST1yLlcXnkXcG+1PZSZ05dATgCrgVXAsab6rcqny2Y1Orpi1rPe9XrtHJq9uNg3\nrdkv7dk3rZXUL10FfUT8OBCZ+dWq6GzT7hrwInC82p6tfLpsVrNdv1qv1xgfn+im2YuOfdOa/dKe\nfdPaoPZLuw+nbqdu3gY81fT42YgYq7bXA/uBQ8DaiBiJiNXA5TRO1B4ENsyoK0nqkW6DPoD/anp8\nJ3BvRDwDLAN2Z+b3ge00gvwrwN2ZOQk8DLwxIg4AH+BH0z+SpB4YWohfPDLbEgiD+l+qXrBvWjuf\nfnn6W//TsnzsykvnokkLhq+Z1ga1X9otgbAg17qReqVdoEsl8c5YSSqcQS9JhTPoJalwBr0kFc6T\nsVoUPOmqxcygl87BYrnsUmVx6kaSCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMJ5Hb00B7y+\nXguZI3pJKpxBL0mFM+glqXAGvSQVzqCXpMJ51Y2K0urql9rKkT60RFo4HNFLUuG6GtFHxMeAXwWW\nAQ8B+4CdwBTwHLA5M89GxG3AJuA0sDUz90TExcAuYA0wAdySmeNzfSCSpNY6jugjYgx4K3AtcD3w\nGuBBYEtmrgWGgI0RcQlwR1VvHbAtIpYDtwOHq7qPA1vm4TgkSW10M3WzDjgMfBH4J2APcBWNUT3A\nXuAG4GrgYGaezMxjwBHgCuA64IkZdSVJPdLN1M1PAK8D3gn8JPAlYElmTlX7J4DVwCrgWNPPtSqf\nLpvV6OgKhoeXtt1fr9e6aPbitNj7pt2J136dkB2Ev8cgtLEfSuqXboL+BeA/M/MUkBExSWP6ZloN\neBE4Xm3PVj5dNqujR0+03Vev1xgfn+ii2YuPfQMTL02+oqy2cqRleS8s9L+Hr5nWBrVf2n04dTN1\ncwB4R0QMRcSrgR8Dnqrm7gHWA/uBQ8DaiBiJiNXA5TRO1B4ENsyoK0nqkY4j+urKmbfRCPIlwGbg\ne8COiFgGPA/szswzEbGdRpAvAe7OzMmIeBh4LCIOAKeAm+fpWCRJLXR1eWVmfrRF8fUt6u0Adswo\nOwHcdF6tkyRdMG+YkqTCuQSCNI/8QhItBI7oJalwBr0kFc6gl6TCGfSSVDiDXpIK51U3GkjtrmaR\n9EqO6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMK51o3U\nB37zlHrJEb0kFc6gl6TCGfSSVLiu5ugj4t+B49XD7wH3ATuBKeA5YHNmno2I24BNwGlga2buiYiL\ngV3AGmACuCUzx+f0KCRJbXUc0UfECDCUmWPVv/cBDwJbMnMtMARsjIhLgDuAa4F1wLaIWA7cDhyu\n6j4ObJmnY5EktdDNiP7NwIqIeLKqfxdwFbCv2r8XeDtwBjiYmSeBkxFxBLgCuA64v6nuPZ2ecHR0\nBcPDS9vur9drXTR7cVosfVNbOTKv9fulH3+/xfKaOVcl9Us3QX8CeAD4DPAzNMJ6KDOnqv0TwGpg\nFXCs6edalU+Xzero0RNt99XrNcbHJ7po9uKzmPpm4qXJruvWVo6cU/1+6vXfbzG9Zs7FoPZLuw+n\nboL+O8CRKti/ExEv0BjRT6sBL9KYw691KJ8ukyT1SDdB/37gTcDvRsSraYzQn4yIscx8GlgPfBU4\nBNxXzekvBy6ncaL2ILCh2r8e2D/XB6Fy+SXg0oXrJugfAXZGxAEaV9m8H/gBsCMilgHPA7sz80xE\nbKcR5EuAuzNzMiIeBh6rfv4UcPN8HIgkqbWOQZ+Z7cL5+hZ1dwA7ZpSdAG463wZKki6MN0xJUuEM\nekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKlxX3zAlqTdmW8Rt7MpLe9gS\nlcQRvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcl1eq7/xeWGl+OaKXpMIZ9JJUOINekgpn0EtS\n4bo6GRsRa4BvAjcCp4GdwBTwHLA5M89GxG3Apmr/1szcExEXA7uANcAEcEtmjs/5UUiS2uo4oo+I\ni4BPA/9XFT0IbMnMtcAQsDEiLgHuAK4F1gHbImI5cDtwuKr7OLBl7g9BkjSbbkb0DwCfAj5WPb4K\n2Fdt7wXeDpwBDmbmSeBkRBwBrgCuA+5vqntPN40aHV3B8PDStvvr9Vo3v2ZRGsS+qa0cKeI55tt8\n/W0H8TXTCyX1y6xBHxG3AuOZ+eWImA76ocycqrYngNXAKuBY04+2Kp8u6+jo0RNt99XrNcbHJ7r5\nNYvOoPbNxEuT8/r7aytH5v05emE+/raD+pqZb4PaL+0+nDqN6N8PTEXEDcCVNKZf1jTtrwEvAser\n7dnKp8sknYd2N5a5Tr06mXWOPjPflpnXZ+YY8C3gt4C9ETFWVVkP7AcOAWsjYiQiVgOX0zhRexDY\nMKOuJKmHzufyyjuBeyPiGWAZsDszvw9spxHkXwHuzsxJ4GHgjRFxAPgAcO/cNFuS1K2u17qpRvXT\nrm+xfwewY0bZCeCm822cJOnCecOUJBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxfJaie8SsDpf5w\nRC9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0\nklQ4g16SCmfQS1LhOq5HHxFLgR1AAFPAB4FJYGf1+Dlgc2aejYjbgE3AaWBrZu6JiIuBXcAaYAK4\nJTPH5+FYJEktdPPFI78CkJnXRsQYcB8wBGzJzKcj4lPAxoh4BrgDeAswAhyIiH8GbgcOZ+afRMR7\ngC3Ah+b+UKTFqd0XuoxdeWmPW6KFqmPQZ+Y/RsSe6uHrgBeBG4B9Vdle4O3AGeBgZp4ETkbEEeAK\n4Drg/qa693R6ztHRFQwPL227v16vdfoVi9ZC7pvaypFF+dz90u1rYSG/ZvqppH7p6qsEM/N0RDwG\nvAv4deDGzJyqdk8Aq4FVwLGmH2tVPl02q6NHT7TdV6/XGB+f6KbZi85C75uJlyb78ry1lSN9e+5+\n6ua1sNBfM/0yqP3S7sOp65OxmXkL8LM05usvbtpVozHKP15tz1Y+XSZJ6pGOQR8RvxkRH6sengDO\nAt+o5usB1gP7gUPA2ogYiYjVwOU0TtQeBDbMqCtJ6pFupm7+AfjbiPgX4CLgw8DzwI6IWFZt787M\nMxGxnUaQLwHuzszJiHgYeCwiDgCngJvn40AkSa11czL2f4HfaLHr+hZ1d9CY2mkuOwHcdL4NlCRd\nGG+YkqTCdXXVjXQu2l3XLak/HNFLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCucN\nU1Kh/EISTXNEL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4byOXufNLxiRBoMjekkqnEEv\nSYUz6CWpcLPO0UfERcCjwGXAcmAr8G1gJzAFPAdszsyzEXEbsAk4DWzNzD0RcTGwC1gDTAC3ZOb4\n/ByKJKmVTiP69wIvZOZa4B3AJ4EHgS1V2RCwMSIuAe4ArgXWAdsiYjlwO3C4qvs4sGV+DkOS1E6n\noP88cE+1PURjtH4VsK8q2wvcAFwNHMzMk5l5DDgCXAFcBzwxo64kqYdmnbrJzJcAIqIG7KYxIn8g\nM6eqKhPAamAVcKzpR1uVT5d1NDq6guHhpW331+u1bn7NotTLvqmtHOnZc12oQWrrfJv5GvH91FpJ\n/dLxOvqIeA3wReChzPxcRNzftLsGvAgcr7ZnK58u6+jo0RNt99XrNcbHJ7r5NYtOr/tm4qXJnj3X\nhaitHBmYtvZC82vE91Nrg9ov7T6cOp2MfRXwJPB7mflUVfxsRIxl5tPAeuCrwCHgvogYoXHS9nIa\nJ2oPAhuq/euB/Rd8JJIuSPONbs0fgn4hSbk6jejvAkaBeyJieq7+Q8D2iFgGPA/szswzEbGdRpAv\nAe7OzMmIeBh4LCIOAKeAm+flKCRJbQ1NTU11rtVj4+MTbRs1qP+l6oVe982gLIHg1E17juhbG9Sc\nqddrQ63KvWFKkgpn0EtS4Qx6SSqcyxSro0GZi5fUmiN6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSS\nVDiDXpIKZ9BLUuEMekkqnHfGSgLa3wHtqpaDzxG9JBXOoJekwhn0klQ45+j1Q65SKZXJEb0kFc6g\nl6TCGfSSVDiDXpIK19XJ2Ii4BvjzzByLiNcDO4Ep4Dlgc2aejYjbgE3AaWBrZu6JiIuBXcAaYAK4\nJTPH5+E4JM0Tb6QafB1H9BHxUeAzwEhV9CCwJTPXAkPAxoi4BLgDuBZYB2yLiOXA7cDhqu7jwJa5\nPwRJ0my6mbr5LvDupsdXAfuq7b3ADcDVwMHMPJmZx4AjwBXAdcATM+pKknqo49RNZn4hIi5rKhrK\nzKlqewJYDawCjjXVaVU+XdbR6OgKhoeXtt1fr9e6+TWL0oX0TW3lSOdKA6rkY7tQ59s3pb8PSzq+\n87lh6mzTdg14EThebc9WPl3W0dGjJ9ruq9drjI9PnENzF48L7ZuJlybnsDULR23lSLHHdqEupG9K\nfh8Oas60+3A6n6tuno2IsWp7PbAfOASsjYiRiFgNXE7jRO1BYMOMupKkHjqfEf2dwI6IWAY8D+zO\nzDMRsZ1GkC8B7s7MyYh4GHgsIg4Ap4Cb56rhOn8udSAtLkNTU1Oda/XY+PhE20YN6n+peqHbvlls\nQe/UTXsX0jclX145qDlTr9eGWpV7w5QkFc6gl6TCuUyxpPPiHbODwxG9JBXOoJekwjl1U7DFdnWN\npNYc0UtS4Qx6SSqcQS9JhTPoJalwnoyVNKe8vn7hMegLMP3Gck0XSa04dSNJhTPoJalwBr0kFc45\nekk94Una/nFEL0mFc0Q/QFy7RtL5MOgl9ZVTOvPPoF+AHLlLmkvO0UtS4RzRS1qQnNKZO/Me9BGx\nBHgIeDNwEvidzDwy3887CJyikdQLvRjR/xowkpm/FBG/CPwFsLEHz7sgGObS3Dqf99Ri/19AL4L+\nOuAJgMz814h4Sw+e84IZ0FI5zvX93G6BwEH9wOhF0K8CjjU9PhMRw5l5ut0P1Ou1odl+Yb1em6u2\ntXXTjW+Y9+eQpF7oxVU3x4HmZF4yW8hLkuZWL4L+ILABoJqjP9yD55QkVXoxdfNF4MaI+BowBLyv\nB88pSaoMTU1N9bsNkqR55J2xklQ4g16SCmfQS1LhFuxaNxGxGthF4zr8ZcBHMvOZ6sqdTwCngScz\n896q/h8Dv1yVfzgzD/Wn5b0TEe8CbsrMm6vH9k0Tl99oiIhrgD/PzLGIeD2wE5gCngM2Z+bZiLgN\n2ETjNbI1M/f0rcE9EBEXAY8ClwHLga3Atym0bxbyiP4jwFOZeT1wK/DXVfmngJtp3HF7TUT8fET8\nAnA9cA3wnqa6xYqITwDbePnf0L55uR8uvwH8EY3lNxaViPgo8BlgpCp6ENiSmWtpXAW3MSIuAe4A\nrgXWAdsiYnk/2ttD7wVeqPrhHcAnKbhvFnLQ/yXw6Wp7GJiMiFXA8sz8bmZOAV8GbqARbE9m5lRm\n/jcwHBH1vrS6d74G3D79wL5p6WXLbwADsfzGHPsu8O6mx1cB+6rtvTReI1cDBzPzZGYeA44AV/S0\nlb33eeCeanuIxmi92L5ZEFM3EfHbwO/PKH5fZn69+kTdBXyYxjTO8aY6E8BPAZPACzPKVwPj89bo\nHpmlb/4+IsaayhZd33ThnJffKE1mfiEiLmsqGqoGAvCj18LMfpouL1ZmvgQQETVgN7AFeKDUvlkQ\nQZ+ZjwCPzCyPiDcBfwf8QWbuq0atzcsp1IAXgVNtygdeu75pYeZSE8X3TRdcfuOVzjZtT78W2r12\nihYRr6FxQ+dDmfm5iLi/aXdRfbNgp24i4udo/Pfq5szcC5CZx4FTEfHTETFEY85sP41lFtZFxJKI\neC2NN/QP+tX2frBvWnL5jVd6tul/gutpvEYOAWsjYqS6COJyGicjixURrwKeBP4wMx+tiovtmwUx\nom9jG40TSJ+ICIBjmbkR+CDwWWApjbnnfwOIiP3AMzQ+vDb3pcX9Z9+8nMtvvNKdwI6IWAY8D+zO\nzDMRsZ1GsC0B7s7MV67RW5a7gFHgnoiYnqv/ELC9xL5xCQRJKtyCnbqRJM0Ng16SCmfQS1LhDHpJ\nKpxBL0mFW8iXV0oLQkQ8SmM5hc8CN9FY9OobwKbMPNXPtkndcEQvdXYr8E4aC2G9lcZaJ4vtngQN\nMEf00iwi4ks0brb6Mo27KI9X5YeB1/azbVK3vGFK6iAipjJzqOlxHfg6cGtmPt23hkldcupGOgcR\ncSnwFPCIIa9BYdBLXYqIN9BYKO2xzPzTfrdH6pZTN1IHETFFY13y/wDuysxdfW6SdE4MeqmDKug/\nAvwZjVUNp30pMz/en1ZJ3TPoJalwztFLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klS4/wdL\nLGEOXKPytgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29eed5d1240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['f2'], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29eec3cf2e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEGCAYAAACD7ClEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29eec3f2438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['f7'], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I also observed the 'target' featuren, which is the label in our model. I found it a extremely imbalanced dataset, with too many positive class(target>=1000). Some solutions to handle this:\n",
    "<br>1. To get more data of negative class (but not feasible in this case)<br/>\n",
    "<br>2. Resampling: up/down sampleing the dataset to ensure almost equal representation for each class in training<br/>\n",
    "<br>3. Weighting/Penalized: Some training algorithms such as tree-based algorithms can take instance weights are parameters. Attach higher weights to instances with less frequent labels.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29ef0cf5c88>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEFCAYAAAASWssjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhSSpyLCQJBUZFpKkov8DwQDTxUHrETIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29ef0f8f358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df['target'], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.target < 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Correlation to check collinearity among features, and some features are in high correlation coefficient. Some impacts of this will be:\n",
    "<br>1. Hard to distinguish which feature is more importance to the result<br/>\n",
    "<br>2. It will be unstable in a regression model if the testing data is in a different correlation.<br/>\n",
    "#### In general, we don't have to take care of this problem. But if necessary, I can drop the less important feature in a high correlation,  use Penalized model, or use PCA or some other methods to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29eeb6ad6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD3CAYAAAAjdY4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEz5JREFUeJzt3X2QXXV9x/H33U0iQcKD4wAa64iIX9BqZjrUQgpCldAK\ndbAWpiNakQELZKwCTq1YFZwpaJ0qaCshKFYszggqjDwI6qi1hoeKSmkq4etgRyMiaG0IxISE7N7+\ncS50Xdmbu3fvPb97T9+vmTPcvWf3nA/Z3U9++Z2nVrvdRpJUv4nSASTp/ysLWJIKsYAlqRALWJIK\nsYAlqZBFw9z4ma3nFT/F4uJt95aOAEBrBM42abdapSMAo/Fncd6yF5WOAMBFWzaUjjAS3w+A3ZYu\nXfAP6Hw657L2j4r/QjgClqRChjoClqQ6TRYf086PBSypMZZMjFcDW8CSGmNyRI5z9MoCltQYTkFI\nUiGOgCWpEEfAklSII2BJKmSxBSxJZTgFIUmFOAUhSYU4ApakQhwBS1IhjbkUOSI+Azzl/01mnjy0\nRJLUpyZNQXweuBA4q6YskrQgTSrgbwBfAfbNzM/VlEeS+jZuc8Ddbsh+Y2aeDbyirjCStBCTrd6X\nUdBtBPx4RNwJHBQRKzrvtYB2Zq4cfjRJmp9BjYAjYgK4FFgBbAdOz8z7Zqx/PfB2YAr4ZGau6Wc/\n3Qr4GGA5sAZY3c/GJalOAzwL4jXAbpl5eEQcBnwIOGHG+r8HXgxsAe6JiM9m5qb57mTOAs7MKWAj\ncPx8NypJJQxwauEI4BaAzLwjIg6dtf4/gL2AnXRmBvrZiQ/llNQYk61Wz8su7AlsnvHxVETMHLD+\nJ/Bd4PtUx8se7ievBSypMSZarZ6XXXgEWDZz05m5EyAiXko1M3AA8Dxg34g4qa+8/XyRJI2i1mSr\n52UXbgWOA+jMAa+fsW4zsA3Y1pmq/TmwTz95vRRZUmNMLpkc1KauA1ZFxG1Uc7ynRsTJwB6ZeXlE\nrAXWRcQO4IfAp/rZiQUsqTF6GNn2JDOngTNnvX3vjPWXAZctdD8WsKTGmBiVKyx6NNQCvnjbvbv+\npCE7Z+nBpSMAcMnWDaUjaIaLtvj9aKLWxHgd1nIELKkxHAFLUiGDmgOuiwUsqTEGeBZELSxgSY3R\nasoTMSRp3ExMehBOkopwDliSCrGAJakQpyAkqZDJxRawJBXRcgQsSWV4JZwkFTJuB+HmHK9HxGER\n8d2IWBcRR8x4/7p6oknS/LQmJ3peRkG3FB8CXgecAXw0Io7tvL/30FNJUh8mF0/0vIyCblMQj2fm\nDwAi4jjgq507wvf19E9JGrZxOw2tW9pHIuKtEfG0zHwQOBm4huohdJI0cgb4TLhadCvg51A9aO4S\ngMxcD7wWuLuGXJI0b+M2B9xtCuJh4NXACyJiRU15JKlvTXoixipgObAGWF1PHEnq37jNAc9ZwJ3n\n3W8Ejq8vjiT1b2LJeF3aMF5pJamLJk1BSNJYaU36SCJJKmJUzm7olQUsqTEmnIKQpDIcAUtSIROL\nx6vShpq21S5/24hLtm4oHQGAs3c/pHQELt52b+kIAJy/14tLR+B9m79fOgIA7Vb5S2KXPLC+dITK\ngS9b8CYcAUtSIRawJBXSmCvhJGnceCGGJBXipciSVIgjYEkqZMJLkSWpDM+CkKRCBlXAETEBXAqs\nALYDp2fmfU/xeZcD/5OZ7+xnP+P114UkddGamOh52YXXALtl5uHAO6meEv9rIuIM4CULyWsBS2qM\niSWLel524QjgFoDMvAM4dObKiFgJ/B6wdkF5F/LFkjRKBjgC3hPYPOPjqYhYBBARzwLOB96y0LzO\nAUtqjNbEwM6CeARYNuPjiczc2Xl9EvBM4EvA/sDuEXFvZn5qvjuZs4A7Lf/XwCbgOuBaYCdwambe\nPt8dSdLQDa6Ab6V6Kvw1EXEY8OQdizLzo8BHASLiTcDB/ZQvdB8BXwl8Bngu8FXg5cCvOu8d1c/O\nJGmoBnchxnXAqoi4DWgBp0bEycAemXn5oHbSrYCflplXAkTE0ZmZndfTg9q5JA3SoJ4Jl5nTwJmz\n3v6N+7n2O/J9QrcC3hQR7wYuzMxXAkTEG4DHFrJDSRqaRUtKJ5iXbuP151JNRF86473lwBuHmkiS\n+jTAsyBq0W0E/DBV2b4gIlZ03msBJwArhx1MkuZtcAfhatGtgFdRjXjXAKvriSNJC9CUAs7MKWAj\ncHx9cSSpf6MytdArL8SQ1BxjdhDOApbUGIM6Da0uFrCk5nAKQpIKacpBOEkaNwO8GU8tLGBJzeEU\nxP9pt1rD3PxYuXjbb1xGXrtzlh5cOgIwGn8WE9u3lI4AwNUHHlk6Aifef1fpCAAsHcA2Wp4FIUmF\nOAKWpDI8DU2SSvEgnCQVYgFLUhmtRYtLR5gXC1hSc7Q8CCdJZVjAklRG2wKWpEIsYEkqZMyuvrWA\nJTVGe3K8Km280kpSN02ZgoiI2Xe1+ArVgzpbmbljqKkkqR9NKWDg58BjwFaqx9HvD/wAaAPPH340\nSZqnMSvgbmkPA74DnJCZBwB3ZOYBmWn5ShpJ7dZEz8so6DYC/hnwOmBtRNxENfKVpNE1IsXaq25p\nb8rMR4FHgAOB59QTSZL6NDHZ+zICuo2AH4+IO4GDgA3ALyPidqCdmStrSSdJ8zAqUwu96lbAxwDL\ngTXA6nriSNICNOWJGJk5BWwEjq8vjiQtQINGwJI0XixgSSqjPTFelTZeaSWpG0fAklTIgO6GFhET\nwKXACmA7cHpm3jdj/auB9wI7gU9m5sf72c94/XUhSd20JnpfunsNsFtmHg68E/jQEysiYjFwMXAs\ncBTwFxGxXz9xLWBJjTHAS5GPAG4ByMw7gENnrDsEuC8zN3VuTLYOeHk/eS1gSc0xuBHwnsDmGR9P\nRcSiOdY9CuzVT9yhzgG32t4+4gnn7/Xi0hG4eNu9pSMAcM7Sg0tH4JKtG0pHAODE++8qHaFRphnY\nEzEeAZbN+HgiM3fOsW4Z8HA/O/EgnKTGmB7coO9W4NXANRFxGLB+xroNwEER8QxgC9X0w9/3sxML\nWFJjDPDf3NcBqyLiNqr7oZ8aEScDe2Tm5RFxLvBlqmncT2bmT/vZiQUsqTGmB9TAmTkNnDnr7Xtn\nrL8BuGGh+7GAJTVGe8yOO1nAkhpjUCPguljAkhpjygKWpDKcgpCkQqZLB5gnC1hSY4zZANgCltQc\nHoSTpEKmxmwIbAFLaowx69+5CzgiLszMv4mIFwJXAc8CfgK8KTN/UFdASerVAO8FUYtu92Q7vPPf\nDwPnZOZvAWcBHxt6KknqQ3seyyjo5X7Au2fmrQCZeTeweLiRJKk/0+3el1HQbQ74ZRHxRWDPiPhT\n4HrgbKrbr0nSyBmzGYiuI+C7gXOBZwAPAZOd16+vIZckzdtUu93zMgq6jYC3A5+lKt0Pdt5rUT2E\nbuWQc0nSvI3K1EKvuhXwKmA5sAZYXU8cSerfiAxsezZnAWfmFLAROL6+OJLUv+mROb+hN16IIakx\nGjMClqRxM24XYljAkhrj8TG7I7sFLKkxRuX0sl5ZwJIawykISSpkasweidEa5jOUzll0QPG/ji7a\nsqF0BABaI/A388SOX5WOAMD0kqeXjsDZux9SOgIAH9n8vdIRaC9eWjoCALstXdpa6Da++cP/7vkX\n7agDn7ng/S2UI2BJjfH4mF0KZwFLaowpC1iSyvAgnCQVMmanAVvAkprDEbAkFeIcsCQV4lkQklSI\nUxCSVMi0I2BJKsOzICSpEKcgJKmQx8fsbjxzFnBEHAe8ALgB+BTwQuDHwJmZ+e+1pJOkeRjmFERE\nLAWuAvYFHgVOycxfPMXnTQA3AV/MzMu6bXOiy7oLgM8D/wC8JzOfBZxB9ZRkSRo50+12z0sfzgLW\nZ+aRwKeBd8/xeX8L7NPLBrsV8I7MfAAgM/+189+7e88qSfWaard7XvpwBHBL5/XNwDGzPyEiTgSm\nZ3xeV93mgL8TEf8I3BYRVwA3Uj2i/p75JJakugzqSriIOA04Z9bbDwGbO68fBfaa9TW/DZwMnAi8\nt5f9dCvg3wXWAqcAO4GTgHXAx3vZsCTVbVAFnJlXAFfMfC8irgWWdT5cBjw868veCCwHvg48D9gR\nET/KzDlHw90KeDvwVqoDcRuAPYEDgDcAK3v9H5GkuuzYOdSzIG4FjgO+DbwK+NbMlZn5jideR8QF\nwIPdyhe6F/AqqjZfA6zuL68k1WfIN+NZA1wZEeuAHVTTDUTEucB9mXn9fDc4ZwFn5hSwkWreV5JG\n3jALODO3Uk3Fzn7/w0/x3gW9bNMLMSQ1hrejlKRCLGBJKsQClqRCtg/3LIiBs4AlNYYjYEkqxAKe\n4aItG4a5+bHSbrVKR+DqA48sHQGAE++/q3QEPrL5e6UjAPC2vX6ndAQu2dqc39M+7/FQjCNgSY3h\nCFiSChnypcgDZwFLaoypaQtYkopwCkKSCrGAJamQnRawJJXhCFiSCvEsCEkqxBGwJBViAUtSIW0L\nWJLKmB6zAp6Ya0VEPBARr6wzjCQtRLvd7nkZBXMWMPAQcHZEXBkRz68rkCT1a2rndM/LKOg2BbEp\nM18dEa8FPhsRm4Cbgf/q5/HLkjRs7dHo1Z51GwHvA5CZ12bmy4CzgSng2DqCSdJ8jdsURLcR8EsA\nImJNZp6VmRuA5ty5WVLjjNtBuG4F/K2IuBM4KCJWzFyRmSuHG0uS5q9Jp6EdAywH1gCr64kjSf2b\nmhqvSeA5Czgzp4CNwPH1xZGk/jVpBCxJY8UClqRCmnQQTpLGyqicXtYrC1hSY4zbhRgWsKTGGJVL\njHtlAUtqDA/CzdAas/mYYVrywPrSETjx/rtKRxgZ7cVLS0cA4JKt5S8uPXv3Q0pHAOCy9o8WvI3p\nMescR8CSGsMRsCQVMswCjoilwFXAvsCjwCmZ+YtZn/N24GRgGrgoM6/rts1ud0OTpLEyPd3ueenD\nWcD6zDwS+DTw7pkrI2Jv4G3A4VR3jbxkVxu0gCU1xvTUdM9LH44Abum8vpnqfjkz/Qr4MfD0zrLL\nnTgFIakxBnUlXEScBpwz6+2HgM2d148Cez3Fl/4EuAeYBN6/q/1YwJIaoz09NZDtZOYVwBUz34uI\na4FlnQ+XAQ/P+rJXAc8CDuh8/OWIuDUzvz3XfpyCkNQY7empnpc+3Aoc13n9KuBbs9ZvArYB2zPz\nMaqC3rvbBh0BS2qMQY2A57AGuDIi1gE7qM52ICLOBe7LzOsj4hjgjoiYBtYBX+22QQtYUmNMP75j\naNvOzK3ASU/x/odnvD4fOL/XbVrAkhpjyCPggbOAJTWGBSxJhTSmgCNiX+AdVJPNVwBPnIJxemZ+\nvZ54ktS7xhQw1TXP1wB7Up1u8YfAL4AvABawpJEz3aAC3i0zPwHVVSGZub7zemctySRpnqZ3Du8s\niGHoVsBbIuIDVNMOT4uINwOPAFtqSSZJ89SeGq8RcLcr4fYHfkp1w4k/Af6o8/q0GnJJ0rwN+Uq4\nges2An4YOAXYD1gLtIBnUx2MWzn8aJI0P6NSrL3qVsCrgOVUl9+trieOJPWvMQWcmVPARuD4+uJI\nUv/a0z4VWZKKaNJZEJI0Vpp0HrAkjZVxOw3NApbUGI05CCdJ48YClqRCxu0gXKvdHsxTRCVJ8+ND\nOSWpEAtYkgqxgCWpEAtYkgqxgCWpEAtYkgqxgCWpkJG7ECMiFgFfBZ5GdSvMo4GTMvPkAhn2Bu6n\nejDpEuDczLy9UI6NwD5UT6k+JTN/WnOGJ74f+wH/BuyXmY/VkWFWjt2AlwJ3dlbdnpnnFcjwbeBQ\nqj+XCzLzxjoyzMpxFLAe2ET1M7J/Zu5fc4a9gQeBPYDtwBsy88E6MjTBKI6Anw3smZkrgQuA91N/\nzmdTle71wNcy8yjgTcDHCuW4EvhuZr6c6mnV76g7Q+f7MQV8iOoXrW5P/Fn8OfD1zDy6s9RSvrMy\nrAUWZ+bvAycAL6gxw5M5MnMiM1dk5tFUA4U31p0B+BSwPjOPBK4G/qrGDGNvFAv4MuCgiFgL3Aac\nVSoD1XPx1nbeWwTUNuKbleMQ4MLOe8+lelxUrRk634/LgXcBW2vc/6/loBp1LY+Ib0TElyIiCmQ4\nBfhpRNwEfBy4ocYMT+bofE+IiNcCmzLzK3VnAN5C9eBeqAr58RozjL1RLODVwD2ZeUZmXg2UuFZ6\nZoZtEbE/1cizztHW7BxTEfF14C+B6+rOAPwMuCkz765x30+V4xTg/Zn5B8BFVN+XujPspBr1/jHw\nd8A/1ZjhyRyZeUbn4/OA95XIAJwIHBsR91CNfq+oOcdYG8UCHikR8RLga8C7MvObJbNk5iuAI4Ev\nFNj9G4DTIuJfqP5lUOdoa6bvAF8EyMx1wLMjolVzhl8CN2Zmu/Mz8cKa9/+kiHgR8HBm3lcowvnA\nBzPzRcCxlPnZHFsjdxBulHR+uD8H/FnBkR8RcR5wf2b+M7CFai62Vpn55DxnRPyI6pethPOpCvCD\nEbEC+Elm1v2vpHXAccAXOhk21rz/mY4Bbi64/03A5s7rn1NNQ6hHFnB376c64v2RzlTj5sw8oUCO\nTwJXRsRpwCRwaoEMo+IDwFURcTzVVMCbCmT4OLAmIu4AWsCZBTI8IajmxUt5D/CJiFgNLAbeXDDL\n2PF2lJJUiHPAklSIBSxJhVjAklSIBSxJhVjAklSIBSxJhVjAklTI/wLQN594b/p5OgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29eeb68dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = df[['f1','f2','f3','f4','f5','f6','f7','f8']].corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem and Model selection\n",
    "#### According to the dataset, the problem can be treated as regressiong problem or classification problem, and I will do both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold cross validation to evaluate the model.\n",
    "<br>I used 5-fold cross-validation method to get the average accuracy of the random forest model.In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chiroy\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    kf = KFold(len(y),n_folds=5,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    clf = clf_class(**kwargs)\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        \n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model\n",
    "#### In this case, the regression model is used for simply predicting the target value, based on features to identify the product is defective of not. I used linear regression and ridge regression in addition as my model. The pros and cons are:\n",
    "<br>Pros: Linear Regression is great if the data follow a linear trend or have a strong linear component<br/>\n",
    "<br>Cons: Linear Regression is limited to linear relationships, so if the dataset is not in linear relationships, the model won't work well. And Linear Regression is sensitive to outliers<br/>\n",
    "#### And the Ridge Regression is a Penalized model to reduce the impact of collinearity of features. According to this specific dataset, I will try this model as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling datasets - with classes ratio of 1:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.735471</td>\n",
       "      <td>8.572519</td>\n",
       "      <td>20.861723</td>\n",
       "      <td>-1.903808</td>\n",
       "      <td>19.639279</td>\n",
       "      <td>13.466934</td>\n",
       "      <td>-1.903808</td>\n",
       "      <td>22.084168</td>\n",
       "      <td>973.415683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.112224</td>\n",
       "      <td>112.777920</td>\n",
       "      <td>24.749499</td>\n",
       "      <td>-0.901804</td>\n",
       "      <td>36.673347</td>\n",
       "      <td>1.643287</td>\n",
       "      <td>0.345671</td>\n",
       "      <td>12.825651</td>\n",
       "      <td>1177.091969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.923848</td>\n",
       "      <td>12.467986</td>\n",
       "      <td>32.585170</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>49.699399</td>\n",
       "      <td>10.821643</td>\n",
       "      <td>0.701403</td>\n",
       "      <td>15.470942</td>\n",
       "      <td>1290.467142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.108216</td>\n",
       "      <td>39.749794</td>\n",
       "      <td>22.484970</td>\n",
       "      <td>-4.308617</td>\n",
       "      <td>30.160321</td>\n",
       "      <td>6.412826</td>\n",
       "      <td>-4.308617</td>\n",
       "      <td>14.809619</td>\n",
       "      <td>998.385759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.945892</td>\n",
       "      <td>32.623211</td>\n",
       "      <td>19.098196</td>\n",
       "      <td>-1.503006</td>\n",
       "      <td>30.661323</td>\n",
       "      <td>16.112224</td>\n",
       "      <td>0.345671</td>\n",
       "      <td>7.535070</td>\n",
       "      <td>1255.201719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1          f2         f3        f4         f5         f6        f7  \\\n",
       "0  17.735471    8.572519  20.861723 -1.903808  19.639279  13.466934 -1.903808   \n",
       "1  -6.112224  112.777920  24.749499 -0.901804  36.673347   1.643287  0.345671   \n",
       "2  11.923848   12.467986  32.585170  0.701403  49.699399  10.821643  0.701403   \n",
       "3   4.108216   39.749794  22.484970 -4.308617  30.160321   6.412826 -4.308617   \n",
       "4  22.945892   32.623211  19.098196 -1.503006  30.661323  16.112224  0.345671   \n",
       "\n",
       "          f8       target  \n",
       "0  22.084168   973.415683  \n",
       "1  12.825651  1177.091969  \n",
       "2  15.470942  1290.467142  \n",
       "3  14.809619   998.385759  \n",
       "4   7.535070  1255.201719  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg = df[df.target < 1000]\n",
    "df_pos = df[df.target >= 1000].sample(n=len(df_neg)*2)\n",
    "df_new = df_neg.append(df_pos).sample(frac=1)\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing - Standardization and dropping 'f7' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X1 = df_new.drop('f7',axis=1)\n",
    "X1 = X1.as_matrix().astype(float)\n",
    "y1 = df_new['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X1 = scaler.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of models\n",
    "<br>Both models perform similar in accuracy, but linear regression has lower root mean square error.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Accuracy (evaluation): 1.0, RMS = 2.97129124931e-13\n",
      "Ridge Accuracy (evaluation): 1.0, RMS = 1.46598381482\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "Ridge_CV_result = run_cv(X1,y1,Ridge) \n",
    "LR_CV_result = run_cv(X1,y1,LinearRegression)\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    rms = np.sqrt(((y_true-y_pred)**2).mean())\n",
    "    y_true = (y_true >= 1000)\n",
    "    y_pred = (y_pred >= 1000)\n",
    "    return [np.mean(y_true == y_pred),rms]\n",
    "\n",
    "print (\"Linear Regression Accuracy (evaluation): \" + str(accuracy(y1, LR_CV_result)[0]) + ', RMS = ' + str(accuracy(y1, LR_CV_result)[1]))\n",
    "print (\"Ridge Accuracy (evaluation): \" + str(accuracy(y1, Ridge_CV_result)[0]) + ', RMS = ' + str(accuracy(y1, Ridge_CV_result)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy\n",
    "<br>I collected testing data from the whole dataset (not from downsampled training dataset), and two models perform similar again. The F1-Score of linear regression is about 0.88. The recall is not as good as precision, this is because there are too many positive class data in the dataset, and we don't train enough positive class data. That is one of the impacts if we downsample the dataset.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Accuracy: 0.779411764706, RMS = 179.718929167\n",
      "Ridge Accuracy: 0.779411764706, RMS = 178.915058464\n"
     ]
    }
   ],
   "source": [
    "X1_test = df.sample(n=int(0.2*len(df_new)))\n",
    "y1_test = X1_test['target']\n",
    "X1_test = X1_test.drop('f7',axis=1)\n",
    "X1_test = X1_test.as_matrix().astype(float)\n",
    "X1_test = scaler.fit_transform(X1_test)\n",
    "\n",
    "LR = LinearRegression()\n",
    "Rdg = Ridge()\n",
    "\n",
    "LR.fit(X1, y1)\n",
    "Rdg.fit(X1, y1)\n",
    "\n",
    "y1_pred_LR = LR.predict(X1_test)\n",
    "y1_pred_Rdg = Rdg.predict(X1_test)\n",
    "\n",
    "print (\"Linear Regression Accuracy: \" + str(accuracy(y1_test, y1_pred_LR)[0]) + ', RMS = ' + str(accuracy(y1_test,y1_pred_LR)[1]))\n",
    "print (\"Ridge Accuracy: \" + str(accuracy(y1_test, y1_pred_Rdg)[0]) + ', RMS = ' + str(accuracy(y1_test, y1_pred_Rdg)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall =  0.779411764706\n",
      "Precision =  1.0\n",
      "F1-Score =  0.876033057851\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y1_test>=1000, y1_pred_LR>=1000).ravel()\n",
    "recall = tp / (tp + fn)\n",
    "prec = tp / (tp + fp)\n",
    "F1 = 2 * recall * prec / (recall + prec)\n",
    "print('Recall = ', recall)\n",
    "print('Precision = ', prec)\n",
    "print('F1-Score = ', F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "#### In this case, the target value will be transformed into label value first and training after that. I used Random Forest as my classification model, and the pros and cons are:\n",
    "<br>Pros: Comparing to other classification model, Random forests are extremely flexible and have very high accuracy.And it works well in a high dimension feature map with missing value (assigned it as a specific value).Decision trees often perform well on imbalanced datasets. The splitting rules that look at the class variable used in the creation of the trees, can force both classes to be addressed. Also, it is not sensitive to linearity of features and outliers.<br/>\n",
    "<br>Cons: The main disadvantage of Random forests is their complexity. They are much harder and time-consuming to construct than decision trees. They also require more computational resources and are also less intuitive. <br/>\n",
    "#### According to this dataset, Random Forest would be a good choice for this classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing - Standardization and dropping 'f7' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('f7',axis=1)\n",
    "X = X.as_matrix().astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of models - adding weigt into training\n",
    "<br>As the model is only trained on the normal/valid data, the majority class, the skewdness of the data is not the problem. For Random Forest, this imbalance has to be taken into account. One possible way would be to sample the data to get a more balanced dataset, another is the introduction of weighted cost for the classes.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Accuracy (evaluation): 0.99999\n"
     ]
    }
   ],
   "source": [
    "y2 = df['target'] >= 1000\n",
    "\n",
    "def accuracy2(y_true,y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_CV_result = run_cv(X,y2,RandomForestClassifier,class_weight = {0: 100000, 1: 1})\n",
    "print(\"Random forest Accuracy (evaluation): \" + str(accuracy2(y2, RF_CV_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy\n",
    "<br>Although I adding a much higher weight to anormaly class, the model still perform well in the testing set(with train- test ratio of 7:3). One reason is the model is trained perfectly. Another reason is still, the data is imbalance, and adding more data will be better for furture prediction.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y2,test_size=0.3)\n",
    "RF = RandomForestClassifier(class_weight={0: 100000, 1: 1})\n",
    "RF.fit(X_train, y_train)\n",
    "y_pred = RF.predict(X_test)\n",
    "print(\"Random forest Accuracy: \" + str(accuracy2(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy is 1.0\n",
      "Precision is 1.0\n",
      "Recall is 1.0\n",
      "F1-score is 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAAEOCAYAAAAXAKWIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVTz3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29ef2f7d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def cal_evaluation(classifier, cm):\n",
    "    tp = cm[0][0]\n",
    "    fp = cm[0][1]\n",
    "    fn = cm[1][0]\n",
    "    tn = cm[1][1]\n",
    "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
    "    precision = tp / (tp + fp + 0.0)\n",
    "    recall = tp / (tp + fn + 0.0)\n",
    "    print(classifier)\n",
    "    print(\"Accuracy is \" + str(accuracy))\n",
    "    print(\"Precision is \" + str(precision))\n",
    "    print(\"Recall is \" + str(recall))\n",
    "    print(\"F1-score is \" + str((2*precision*recall)/(precision+recall)))\n",
    "\n",
    "def draw_confusion_matrices(confusion_matricies,class_names):\n",
    "    class_names = class_names.tolist()\n",
    "    for cm in confusion_matrices:\n",
    "        classifier, cm = cm[0], cm[1]\n",
    "        cal_evaluation(classifier, cm)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n",
    "        plt.title('Confusion matrix for %s' % classifier)\n",
    "        fig.colorbar(cax)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "    \n",
    "y = np.array(y)\n",
    "class_names = np.unique(y)\n",
    "\n",
    "confusion_matrices = [\n",
    "    (\"Random Forest\", confusion_matrix(y_test,y_pred)),\n",
    "]\n",
    "\n",
    "draw_confusion_matrices(confusion_matrices,class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance of Features - Another Pros of Random Forest is, we can find which feature has more impact on final result\n",
    "<br>And we could see, 'f8' has much higher importance than other features.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance ranking by Random Forest Model:\n",
      "f8: 0.9283\n",
      "f4: 0.03\n",
      "f7: 0.0221\n",
      "f3: 0.009\n",
      "f6: 0.0041\n",
      "f1: 0.0031\n",
      "f2: 0.0021\n",
      "f5: 0.0014\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(X, y2)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature importance ranking by Random Forest Model:\")\n",
    "for k,v in sorted(zip(map(lambda x: round(x, 4), importances), df.columns), reverse=True):\n",
    "    print(v + \": \" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "#### Depending on this specific dataset, the problem can treated as a anomaly detection problem as well. There are many algorithm for Anomaly Detection: Density-based techniques like KNN, Clustering-Based techniques,SVM, Bayesian Networks, etc. So I will use KNN in this case.\n",
    "<br>Pros: Very easy to understand and implement. Does not assume any probability distributions on the input data. Can quickly respond to changes in input(which is good for anomaly detection).<br/>\n",
    "<br>Cons: Sensitive to localized data. And if in high dimensions, KNN may consider outliers in some dimensions as anomal data points.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Accuracy\n",
    "<br>We could see KNN performed as good as Random Forest, with F1-Score of 1. But in genearl, Anomaly Detection may combine two predictive models (KNN and another one), in case of outliers in some dimensions and KNN may consider them as anomal data points.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest Neighbors Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "X = df.drop('f7',axis=1)\n",
    "X = X.as_matrix().astype(float)\n",
    "y2 = df['target'] >= 1000\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y2,test_size=0.3)\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(X_train, y_train)\n",
    "y_pred = RF.predict(X_test)\n",
    "print(\"K-nearest Neighbors Accuracy: \" + str(accuracy2(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall =  1.0\n",
      "Precision =  1.0\n",
      "F1-Score =  1.0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "recall = tp / (tp + fn)\n",
    "prec = tp / (tp + fp)\n",
    "F1 = 2 * recall * prec / (recall + prec)\n",
    "print('Recall = ', recall)\n",
    "print('Precision = ', prec)\n",
    "print('F1-Score = ', F1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
